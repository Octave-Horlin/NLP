{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Octave-Horlin/NLP/blob/main/LabTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbd55myMKQq2"
      },
      "source": [
        "# Hello world Transformers üëê\n",
        "\n",
        "In this notebook we will explore the basics of the Hugging Face library by using a pre-trained model to classify text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJsBgakKQq4"
      },
      "source": [
        "## Quick overview of Transformer applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzPnEB2BKQq5"
      },
      "source": [
        "Let's start by defining a text that we will use to test the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6uxWMDkKQq5"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
        "from your online store in Germany. Unfortunately, when I opened the package,\n",
        "I discovered to my horror that I had been sent an action figure of Megatron\n",
        "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
        "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
        "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
        "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYvSbbVqKQq7"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "### Question 1: Understanding Pipelines\n",
        "\n",
        "Before we start using the models, let's understand what we're working with:\n",
        "\n",
        "1. What is a `pipeline` in Hugging Face Transformers? What does it abstract away from the user?\n",
        "\n",
        "2. Visit the `pipeline` documentation and list at least 3 other tasks (besides text-classification) that are available.\n",
        "\n",
        "3. What happens when you don't specify a model in the `pipeline`? How can you specify a specific model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_yzoxwFKQq8"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. A `pipeline` in Hugging Face Transformers is a high-level abstraction that encapsulates the entire process necessary for an NLP task. It abstracts away the complexity of:\n",
        "   - Managing the tokenizer (text tokenization)\n",
        "   - Loading and initializing the model\n",
        "   - Handling input/output formats\n",
        "   - Post-processing the results\n",
        "   - Managing the device (CPU/GPU)\n",
        "\n",
        "2. Here are at least 3 other tasks available with `pipeline` (besides text-classification):\n",
        "   - `sentiment-analysis`\n",
        "   - `translation`\n",
        "   - `summarization`\n",
        "   - `question-answering`\n",
        "   - `named-entity-recognition` (NER)\n",
        "   - `text-generation`\n",
        "\n",
        "3. When you don't specify a model in the `pipeline`, Hugging Face automatically uses a default model for the requested task. For example, for `text-classification`, it uses `distilbert-base-uncased-finetuned-sst-2-english` by default. To specify a specific model, you can use:\n",
        "   ```python\n",
        "   classifier = pipeline(\"text-classification\", model=\"model-name\")\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C_ofGWiKQq9",
        "outputId": "1a76ee1d-2a66-4d45-e750-934a537323b1",
        "colab": {
          "referenced_widgets": [
            "cc2457d3a6324007ba94ba18b62275f9",
            "afd96ceaa6c04ec4a0866131701095c7",
            "154cd786c63245248e6a21e42cc00179",
            "3ae0d225ad654ef38604873cb4f4f008"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2457d3a6324007ba94ba18b62275f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afd96ceaa6c04ec4a0866131701095c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "154cd786c63245248e6a21e42cc00179",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ae0d225ad654ef38604873cb4f4f008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"text-classification\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF-RynG4KQq-"
      },
      "source": [
        "### Question 2: Text Classification Deep Dive\n",
        "\n",
        "Now that you've seen text classification in action, explore further:\n",
        "\n",
        "1. What is the default model used for text-classification? Look at the output above to find its name, then search for it on the Hugging Face Model Hub.\n",
        "\n",
        "2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
        "\n",
        "3. The output includes a `score` field. What does this score represent? What range of values can it have?\n",
        "\n",
        "4. Challenge: Find a different text-classification model on the Hub that classifies emotions (not just positive/negative). What is its name?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igQ9GyvkKQq_"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. The default model used for text-classification is `distilbert-base-uncased-finetuned-sst-2-english`. This is a DistilBERT model that has been fine-tuned specifically for sentiment analysis.\n",
        "\n",
        "2. This model was fine-tuned on the **SST-2 (Stanford Sentiment Treebank v2)** dataset. This dataset contains movie reviews labeled as positive or negative. The model works best with English text that expresses sentiment or opinion, such as reviews, feedback, or opinionated statements.\n",
        "\n",
        "3. The `score` field represents the **confidence/probability** that the text belongs to the predicted label. It is a value between 0 and 1, where values closer to 1 indicate higher confidence in the prediction. For binary classification, the scores for both labels typically sum to approximately 1.0.\n",
        "\n",
        "4. One example of a text-classification model for emotions is `j-hartmann/emotion-english-distilroberta-base`, which classifies text into emotions such as joy, sadness, anger, fear, surprise, and disgust. Another popular one is `bhadresh-savani/bert-base-uncased-emotion`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVoLPDnEKQq_",
        "outputId": "84d168d2-a418-4212-e8d6-7f8f8a2bcd89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.901546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label     score\n",
              "0  NEGATIVE  0.901546"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4rxiabSKQrA"
      },
      "source": [
        "## Named Entity Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Bc7Z3ZKQrA",
        "outputId": "e07627ee-dbfe-4e0e-cfbe-e34baf021a25",
        "colab": {
          "referenced_widgets": [
            "4b07ca4344944f7099161dc1ec2e5df0",
            "a3943ef7b1fd489f85796f93afb831e2",
            "181d5d46233a47778b9f631144c72d32",
            "be085507d8fe4c7bb0cadea2ec6fc46a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b07ca4344944f7099161dc1ec2e5df0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3943ef7b1fd489f85796f93afb831e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "181d5d46233a47778b9f631144c72d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be085507d8fe4c7bb0cadea2ec6fc46a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity_group</th>\n",
              "      <th>score</th>\n",
              "      <th>word</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.879010</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.990859</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>36</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>0.999755</td>\n",
              "      <td>Germany</td>\n",
              "      <td>90</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.556570</td>\n",
              "      <td>Mega</td>\n",
              "      <td>208</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.590256</td>\n",
              "      <td>##tron</td>\n",
              "      <td>212</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.669693</td>\n",
              "      <td>Decept</td>\n",
              "      <td>253</td>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.498349</td>\n",
              "      <td>##icons</td>\n",
              "      <td>259</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.775362</td>\n",
              "      <td>Megatron</td>\n",
              "      <td>350</td>\n",
              "      <td>358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>367</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.812096</td>\n",
              "      <td>Bumblebee</td>\n",
              "      <td>502</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  entity_group     score           word  start  end\n",
              "0          ORG  0.879010         Amazon      5   11\n",
              "1         MISC  0.990859  Optimus Prime     36   49\n",
              "2          LOC  0.999755        Germany     90   97\n",
              "3         MISC  0.556570           Mega    208  212\n",
              "4          PER  0.590256         ##tron    212  216\n",
              "5          ORG  0.669693         Decept    253  259\n",
              "6         MISC  0.498349        ##icons    259  264\n",
              "7         MISC  0.775362       Megatron    350  358\n",
              "8         MISC  0.987854  Optimus Prime    367  380\n",
              "9          PER  0.812096      Bumblebee    502  511"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q88ZJGecKQrA"
      },
      "source": [
        "### Question 3: Named Entity Recognition (NER)\n",
        "\n",
        "Let's understand NER better:\n",
        "\n",
        "1. What does the `aggregation_strategy=\"simple\"` parameter do in the NER pipeline? Check the token classification documentation.\n",
        "\n",
        "2. Looking at the output above, what do the entity types mean? (ORG, MISC, LOC, PER)\n",
        "\n",
        "3. Why do some words appear with `##` prefix (like `##tron` and `##icons`)? What does this indicate about tokenization?\n",
        "\n",
        "4. The model seems to have split \"Megatron\" and \"Decepticons\" incorrectly. Why might this happen? What does this tell you about the model's training data?\n",
        "\n",
        "5. **Challenge:** Find the model card for `dbmdz/bert-large-cased-finetuned-conll03-english`. What is the CoNLL-2003 dataset?\n",
        "\n",
        "ü§î How might the choice of tokenizer affect NER performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLzzRZI8KQrB"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. The `aggregation_strategy=\"simple\"` parameter groups consecutive tokens that have the same entity label. Without this parameter, NER would return one prediction per token (subword), which can be fragmented. With \"simple\", it combines tokens with the same label into a single entity, making the output more readable and useful.\n",
        "\n",
        "2. The entity types represent different categories:\n",
        "   - **ORG**: Organization (e.g., companies, institutions)\n",
        "   - **MISC**: Miscellaneous (other named entities that don't fit other categories)\n",
        "   - **LOC**: Location (geographical locations like cities, countries)\n",
        "   - **PER**: Person (names of people)\n",
        "\n",
        "3. The `##` prefix indicates that these are **subword tokens** that are part of a larger word. This happens with WordPiece tokenization (used by BERT models), where words are split into smaller pieces. The `##` prefix means this token is a continuation of the previous token, not the start of a new word.\n",
        "\n",
        "4. \"Megatron\" and \"Decepticons\" were likely split incorrectly because these are fictional names from Transformers that weren't present in the model's training data (CoNLL-2003). The model trained on real-world entities, so when it encounters unknown proper nouns, it tries to apply its learned patterns, which can lead to incorrect tokenization and entity recognition for fictional or domain-specific terms.\n",
        "\n",
        "5. The **CoNLL-2003** dataset is a standard benchmark dataset for Named Entity Recognition. It contains news articles annotated with four entity types (PER, LOC, ORG, MISC). The dataset is in English and German, with training, validation, and test sets. It was created for the CoNLL-2003 shared task on language-independent named entity recognition.\n",
        "\n",
        "The choice of tokenizer can significantly affect NER performance because:\n",
        "- Different tokenizers split words differently, which can break entity boundaries\n",
        "- Subword tokenization (WordPiece, BPE) can fragment multi-word entities\n",
        "- The tokenizer should match the one used during training for best results\n",
        "- Some tokenizers preserve more word-level information, which can help with entity recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Vs9rH7KQrB"
      },
      "source": [
        "## Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKaHMPt4KQrC",
        "outputId": "671de2fd-57e8-47d0-9c1c-f5f0dd5ebd52",
        "colab": {
          "referenced_widgets": [
            "0e2f30d5a02c4c639fef234d7c2f14ad",
            "8c5821c2dba84197ad7ce3ac9bdacf63",
            "a43b00829fb84f26bebfffbdefa87063",
            "7ebaa10b58834e6197a2b766ed1c3bc5",
            "66b38f4664be417c88649d23b3f1a9b7"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e2f30d5a02c4c639fef234d7c2f14ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c5821c2dba84197ad7ce3ac9bdacf63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a43b00829fb84f26bebfffbdefa87063",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ebaa10b58834e6197a2b766ed1c3bc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66b38f4664be417c88649d23b3f1a9b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.631292</td>\n",
              "      <td>335</td>\n",
              "      <td>358</td>\n",
              "      <td>an exchange of Megatron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      score  start  end                   answer\n",
              "0  0.631292    335  358  an exchange of Megatron"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reader = pipeline(\"question-answering\")\n",
        "question = \"What does the customer want?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "pd.DataFrame([outputs])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yztm22AtKQrC"
      },
      "source": [
        "### Question 4: Question Answering Systems\n",
        "\n",
        "Explore how question answering works:\n",
        "\n",
        "1. What type of question answering is this? (Extractive vs. Generative) Check the question answering documentation.\n",
        "\n",
        "2. The model outputs `start` and `end` indices. What do these represent? Why are they important?\n",
        "\n",
        "3. What is the SQuAD dataset? (Look up the model `distilbert-base-cased-distilled-squad` on the Hub)\n",
        "\n",
        "4. Try to think of a question this model CANNOT answer based on the text. Why would it fail?\n",
        "\n",
        "5. **Challenge:** What's the difference between extractive and generative question answering? Find an example of a generative QA model on the Hub.\n",
        "\n",
        "üí° **Hint:** Try asking questions that require reasoning or information not in the text. What happens?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpImmIOKQrC"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. This is **extractive question answering**. The model extracts a span of text directly from the provided context document to answer the question, rather than generating a new answer. The model identifies the start and end positions in the context where the answer can be found.\n",
        "\n",
        "2. The `start` and `end` indices represent the **character positions** in the original context text where the answer begins and ends. They are important because:\n",
        "   - They allow you to locate the exact answer within the context\n",
        "   - They enable highlighting or extracting the answer span\n",
        "   - They provide precise positioning for downstream applications\n",
        "   - They help verify that the model found a valid answer within the provided context\n",
        "\n",
        "3. **SQuAD (Stanford Question Answering Dataset)** is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answers to these questions are spans of text from the corresponding reading passage. SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles. The model `distilbert-base-cased-distilled-squad` was fine-tuned on this dataset to perform extractive question answering.\n",
        "\n",
        "4. This model cannot answer questions that:\n",
        "   - Require information not present in the context text (e.g., \"What is the customer's email address?\" if it's not in the text)\n",
        "   - Need reasoning beyond simple extraction (e.g., \"Why did the customer choose Amazon?\" requires inference)\n",
        "   - Ask for opinions, predictions, or hypotheticals (e.g., \"What should Amazon do next?\")\n",
        "   - Require combining information from multiple parts of the text with complex logic\n",
        "\n",
        "   It would fail because extractive QA can only copy text from the context and cannot generate new information or perform complex reasoning.\n",
        "\n",
        "5. **Extractive QA** finds and extracts a span of text from the context document. The answer must exist verbatim in the context. Examples include BERT-based models fine-tuned on SQuAD.\n",
        "\n",
        "   **Generative QA** generates a new answer that may not appear exactly in the context, using language generation capabilities. Examples include:\n",
        "   - `google/flan-t5-base` (can be used for generative QA)\n",
        "   - `microsoft/DialoGPT-medium` (dialogue-based QA)\n",
        "   - `facebook/blenderbot-400M-distill` (conversational QA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIFuJe1cKQrC"
      },
      "source": [
        "## Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhZWzwEKQrC"
      },
      "source": [
        "### Question 5: Text Summarization\n",
        "\n",
        "Before running the summarization code, let's understand how it works:\n",
        "\n",
        "1. What is the difference between extractive and abstractive summarization? Check the summarization documentation.\n",
        "\n",
        "2. Looking at the code in the next cell, what is the default model used for summarization? Search for it on the Hugging Face Model Hub and determine:\n",
        "   - Is it an extractive or abstractive model?\n",
        "   - What architecture does it use? (Hint: look at the model name)\n",
        "   - What dataset was it trained on?\n",
        "\n",
        "3. What do the `max_length` and `min_length` parameters control? What happens if `min_length > max_length`?\n",
        "\n",
        "4. The parameter `clean_up_tokenization_spaces=True` is used. What does this parameter do? Why might it be useful for summarization?\n",
        "\n",
        "5. **Challenge:** Find two different summarization models on the Hub:\n",
        "   - One optimized for short texts (like news articles)\n",
        "   - One that can handle longer documents\n",
        "   - Compare their architectures and training data.\n",
        "\n",
        "üí° Why might summarization be more challenging than text classification? What linguistic capabilities does the model need?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAigA9evKQrC"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. **Extractive summarization** selects and combines existing sentences or phrases directly from the source text to create a summary. It doesn't generate new text, just extracts the most important parts.\n",
        "\n",
        "   **Abstractive summarization** generates new sentences that may not appear in the original text, paraphrasing and condensing the information. It requires understanding the content and expressing it in a different way.\n",
        "\n",
        "2. The default model is `sshleifer/distilbart-cnn-12-6`:\n",
        "   - It is an **abstractive** model (generates new text)\n",
        "   - It uses the **BART** architecture (Bidirectional and Auto-Regressive Transformers), specifically a distilled version\n",
        "   - It was trained on the **CNN/DailyMail** dataset, which contains news articles and their summaries\n",
        "   - The \"12-6\" in the name refers to 12 encoder layers and 6 decoder layers\n",
        "\n",
        "3. The `max_length` parameter controls the **maximum number of tokens** in the generated summary, while `min_length` controls the **minimum number of tokens**. If `min_length > max_length`, it creates an impossible constraint - the model cannot generate a summary that is both longer than the maximum and shorter than the minimum. This will cause a warning and the model will generate up to `max_length`, ignoring the `min_length` constraint.\n",
        "\n",
        "4. The `clean_up_tokenization_spaces=True` parameter removes extra spaces that can be introduced during tokenization. For example, subword tokenization might add spaces around punctuation or between tokens. This parameter ensures the output text is clean and readable, which is especially important for summarization where the output should be natural and well-formatted.\n",
        "\n",
        "5. Examples of summarization models:\n",
        "   - **Short texts (news articles):** `facebook/bart-large-cnn` - Optimized for CNN/DailyMail news articles, uses BART architecture\n",
        "   - **Longer documents:** `google/pegasus-xsum` - Can handle longer documents, uses PEGASUS architecture trained on XSum dataset\n",
        "   - Another option for long documents: `allenai/led-large-16384` - Uses Longformer architecture that can handle up to 16,384 tokens\n",
        "\n",
        "Summarization is more challenging than text classification because:\n",
        "- It requires **generation** of new text, not just classification\n",
        "- It needs to understand the **entire document** and identify key information\n",
        "- It must maintain **coherence** and **fluency** in the generated summary\n",
        "- It requires **compression** skills to condense information while preserving meaning\n",
        "- It needs to handle **long-range dependencies** across the entire document\n",
        "- It must balance between being concise and informative\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXjJbgrBKQrC",
        "outputId": "31c81d6e-9c4e-414a-ded7-b6e1d5e3a9c7",
        "colab": {
          "referenced_widgets": [
            "a7eae413bff746b696f5dce768304c0b",
            "5a8c1a5da1bb487b98d29150bd33fc28",
            "8d91f4b07e2e4f57b39bbfc56461f1dc",
            "0b150a9176f44dac9a2c7051cbe19954",
            "2b16958da4a945bfa129d1fafdcccfc1"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7eae413bff746b696f5dce768304c0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a8c1a5da1bb487b98d29150bd33fc28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Exception ignored in: <function tqdm.__del__ at 0x000001FA5095C180>\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
            "    self.close()\n",
            "  File \"c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
            "    self.disp(bar_style='danger', check_delay=False)\n",
            "    ^^^^^^^^^\n",
            "AttributeError: 'tqdm' object has no attribute 'disp'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d91f4b07e2e4f57b39bbfc56461f1dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b150a9176f44dac9a2c7051cbe19954",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b16958da4a945bfa129d1fafdcccfc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "outputs = summarizer(text, max_length=40, min_length=10, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNpGlnrdKQrD"
      },
      "source": [
        "## Machine Translation\n",
        "\n",
        "### Question 6: Machine Translation\n",
        "\n",
        "Let's explore how translation models work:\n",
        "\n",
        "1. What is the architecture behind the `Helsinki-NLP/opus-mt-en-de` model? Look it up on the Model Hub.\n",
        "   - What does 'OPUS' stand for?\n",
        "   - What does 'MT' stand for?\n",
        "\n",
        "2. How would you find a model to translate from English to French? Visit the translation documentation and the Model Hub to find at least 2 different models.\n",
        "\n",
        "3. What is the difference between **bilingual** and **multilingual** translation models? What are the advantages and disadvantages of each?\n",
        "\n",
        "4. In the code, we specify the task as `translation_en_to_de`. How does this relate to the model we're loading?\n",
        "\n",
        "5. The output shows a warning about `sacremoses`. What is this library used for in NLP? Check the MarianMT documentation.\n",
        "\n",
        "6. **Challenge:** Find a multilingual model (like mBART or M2M100) that can translate between multiple language pairs. How many language pairs does it support?\n",
        "\n",
        "üåç What challenges exist for low-resource languages?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTtuI2gYKQrD"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. The `Helsinki-NLP/opus-mt-en-de` model uses the **MarianMT** architecture (Neural Machine Translation framework):\n",
        "   - **OPUS** stands for \"Open Parallel Corpus\" - it's a collection of parallel texts (texts in multiple languages) used for training translation models\n",
        "   - **MT** stands for \"Machine Translation\"\n",
        "\n",
        "2. To find a model for English to French translation, you can:\n",
        "   - Use the Hugging Face Model Hub and search for \"translation_en_to_fr\"\n",
        "   - Examples include:\n",
        "     - `Helsinki-NLP/opus-mt-en-fr` (bilingual, MarianMT architecture)\n",
        "     - `facebook/mbart-large-50-many-to-many-mmt` (multilingual, mBART architecture)\n",
        "     - `t5-base` can also be fine-tuned for translation tasks\n",
        "\n",
        "3. **Bilingual models** are trained specifically for one language pair (e.g., English ‚Üî German):\n",
        "   - Advantages: Usually higher quality for that specific pair, faster inference, smaller model size\n",
        "   - Disadvantages: Need separate models for each language pair, cannot handle multiple languages\n",
        "\n",
        "   **Multilingual models** can translate between multiple language pairs with a single model:\n",
        "   - Advantages: One model handles many languages, can leverage shared representations across languages\n",
        "   - Disadvantages: Larger model size, potentially lower quality for individual pairs, more complex training\n",
        "\n",
        "4. The task `translation_en_to_de` specifies that we want to translate from English (en) to German (de). This task name helps Hugging Face select the appropriate default model and ensures the pipeline knows the translation direction. When we also specify `model=\"Helsinki-NLP/opus-mt-en-de\"`, we're explicitly choosing a model trained for this specific language pair.\n",
        "\n",
        "5. **Sacremoses** is a Python library for sentence segmentation and tokenization, particularly useful for languages that require morphological analysis. It's recommended for MarianMT models because:\n",
        "   - It helps with proper sentence splitting (especially for languages with different punctuation rules)\n",
        "   - It provides better tokenization for certain languages\n",
        "   - It improves translation quality by handling sentence boundaries correctly\n",
        "\n",
        "6. Examples of multilingual translation models:\n",
        "   - **mBART-50**: `facebook/mbart-large-50-many-to-many-mmt` - Supports 50 languages and can translate between any pair of those languages\n",
        "   - **M2M-100**: `facebook/m2m100_418M` - Supports 100 languages and can translate between any pair\n",
        "   - **NLLB-200**: `facebook/nllb-200-3.3B` - Meta's No Language Left Behind model supporting 200 languages\n",
        "\n",
        "Challenges for low-resource languages:\n",
        "- Limited training data available\n",
        "- Fewer parallel corpora (texts translated into those languages)\n",
        "- Lack of quality benchmarks and evaluation datasets\n",
        "- Models often underperform compared to high-resource languages\n",
        "- Domain-specific terminology may be missing\n",
        "- Dialectal variations and informal language are poorly covered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3wN_gDRKQrD",
        "outputId": "128c26a0-7725-4621-f7a5-5870e62cf7e2",
        "colab": {
          "referenced_widgets": [
            "b2b2a999e9d54acdaee1b048d3455637",
            "b4a6b25de470430187fdeeb54dc0bfb7",
            "5b58a9ced422408c9780fd1f391e230c",
            "da94fab4fe77485ba71c78b5ed6fb410",
            "f335c1af5a3b444198b254ffe12243bc",
            "242dbd10016b4a05ac49cd01a14d208a",
            "60ad43e11a834c7980fbd08c58094862"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2b2a999e9d54acdaee1b048d3455637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4a6b25de470430187fdeeb54dc0bfb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b58a9ced422408c9780fd1f391e230c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function tqdm.__del__ at 0x000001FA5095C180>\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
            "    self.close()\n",
            "  File \"c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
            "    self.disp(bar_style='danger', check_delay=False)\n",
            "    ^^^^^^^^^\n",
            "AttributeError: 'tqdm' object has no attribute 'disp'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da94fab4fe77485ba71c78b5ed6fb410",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f335c1af5a3b444198b254ffe12243bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "242dbd10016b4a05ac49cd01a14d208a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60ad43e11a834c7980fbd08c58094862",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket √∂ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie k√∂nnen mein Dilemma verstehen. Um das Problem zu l√∂sen, Ich fordere einen Austausch von Megatron f√ºr die Optimus Prime Figur habe ich bestellt.\n"
          ]
        }
      ],
      "source": [
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNC_D44AKQrD"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WNM84LmKQrD"
      },
      "source": [
        "### Question 7: Text Generation\n",
        "\n",
        "Understand how language models generate text:\n",
        "\n",
        "1. What is the default model used for text generation in the code below? Look it up on the Hub and answer:\n",
        "   - What architecture does GPT-2 use? (decoder-only, encoder-decoder, or encoder-only?)\n",
        "   - How many parameters does the base GPT-2 model have?\n",
        "   - What type of generation does it perform? (autoregressive, non-autoregressive, etc.)\n",
        "\n",
        "2. Why do we use `set_seed(42)` before generation? What would happen without it? Check the generation documentation.\n",
        "\n",
        "3. The code uses `max_length=200`. What other parameters can control text generation? Research and explain:\n",
        "   - `temperature`\n",
        "   - `top_k`\n",
        "   - `do_sample`\n",
        "\n",
        "4. Looking at the output, you can see a warning about truncation. What does this mean? Why is the input being truncated?\n",
        "\n",
        "5. What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
        "\n",
        "6. What are the trade-offs between model size and generation quality?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh35R_bbKQrE"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "1. The default model is **GPT-2** (`openai-community/gpt2`):\n",
        "   - **Architecture:** GPT-2 uses a **decoder-only** Transformer architecture. It has only decoder layers (no encoder), with self-attention mechanisms and feed-forward networks.\n",
        "   - **Parameters:** The base GPT-2 model has **124 million parameters**. There are also larger variants: GPT-2 Medium (355M), GPT-2 Large (774M), and GPT-2 XL (1.5B).\n",
        "   - **Generation type:** GPT-2 performs **autoregressive generation**, meaning it generates text one token at a time, with each new token being conditioned on all previously generated tokens.\n",
        "\n",
        "2. `set_seed(42)` ensures **reproducibility** by fixing the random number generator seed. Without it:\n",
        "   - Each run would produce different outputs (non-deterministic)\n",
        "   - Results would be difficult to reproduce for debugging or comparison\n",
        "   - The generation would be stochastic, making it harder to test and validate\n",
        "   - Setting a seed allows for consistent, reproducible results across runs\n",
        "\n",
        "3. Parameters that control text generation:\n",
        "   - **`temperature`**: Controls randomness in generation. Lower values (0.1-0.7) make output more deterministic and focused, higher values (0.8-1.5) make it more creative/random. Default is 1.0.\n",
        "   - **`top_k`**: Limits sampling to the k most likely next tokens. Reduces randomness by ignoring low-probability tokens. Common values: 50, 100.\n",
        "   - **`do_sample`**: Boolean flag that enables/disables sampling. If `False`, uses greedy decoding (always picks most likely token). If `True`, uses sampling based on probability distribution.\n",
        "\n",
        "4. The truncation warning means the input text is being **cut off** because it exceeds the model's maximum context length. GPT-2 has a maximum sequence length (typically 1024 tokens). If the input + generation length exceeds this, the input must be truncated. This is necessary because:\n",
        "   - Transformers have fixed maximum context windows\n",
        "   - The model needs to fit within memory constraints\n",
        "   - Truncation prevents errors from sequences that are too long\n",
        "\n",
        "5. Setting `pad_token_id` to `eos_token_id` is necessary because:\n",
        "   - GPT-2 doesn't have a dedicated padding token in its vocabulary\n",
        "   - During training, padding is needed for batching sequences of different lengths\n",
        "   - Using the end-of-sequence (EOS) token as padding allows the model to work properly with padded sequences\n",
        "   - Without this, the model might not handle variable-length inputs correctly\n",
        "\n",
        "6. Trade-offs between model size and generation quality:\n",
        "   - **Larger models** (e.g., GPT-2 XL, GPT-3): Better quality, more coherent text, better understanding of context, but require more memory, slower inference, higher cost\n",
        "   - **Smaller models** (e.g., GPT-2 base): Faster inference, less memory, lower cost, but may produce less coherent or less contextually appropriate text\n",
        "   - Generally, quality improves with size up to a point, but with diminishing returns\n",
        "   - Larger models can handle longer contexts and generate more diverse, creative outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_cqNssSKQrE",
        "outputId": "728f261f-5f67-43de-952a-0e7ff17c4779",
        "colab": {
          "referenced_widgets": [
            "075b36dd108b4e02a611a484a68f5066",
            "ca0203b888a6401c9f02785be38603fb",
            "e06c33060b4a40d39ff70b952459e1d9",
            "ffb0e91ea74542359e8efc281ce39039",
            "015aab6ef0354c689c249ce94b9de705",
            "79a53bd4d3e04d43a8c5b2eb50b2b785",
            "55c8321053c64f868359f7cbe803f990"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "075b36dd108b4e02a611a484a68f5066",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\octav\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca0203b888a6401c9f02785be38603fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e06c33060b4a40d39ff70b952459e1d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffb0e91ea74542359e8efc281ce39039",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "015aab6ef0354c689c249ce94b9de705",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79a53bd4d3e04d43a8c5b2eb50b2b785",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55c8321053c64f868359f7cbe803f990",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer service response: Dear Amazon, last week I ordered an Optimus Prime action figure\n",
            "from your online store in Germany. Ugh, I was not able to pick up my order, i am working hard and would like to make an order here. I am so happy because I am looking forward to getting my picture taken with my new figure. Thank you.\n",
            "\n",
            "Hi, I have ordered a lot of Optimus Prime action figures so I'm going to try to order some more. But, I'm not sure what to say, I just hope you will give me a discount on some of the products in my order.\n",
            "\n",
            "Thank you again for your time and understanding.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "S.\n",
            "\n",
            "Dear Santa,\n",
            "\n",
            "Thank you so much for the great thank you! I was told that you would send me another order for my order, but I was told that you would not. I've ordered this figure for my friend to get to see his favorite movie. She is a fan, and I was the only one who could see the movie in the movie theater (she has no idea my boyfriend is in the movie theater). So I am really looking forward to your help! Thanks again,\n",
            "\n",
            "Hi, I have ordered a lot of Optimus Prime action figures so I'm going to try to order some more. But, I'm not sure what to say\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import set_seed\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "set_seed(42)\n",
        "prompt = \"Customer service response: \"\n",
        "outputs = generator(prompt + text[:100], max_length=200, num_return_sequences=1,\n",
        "                   pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "print(\"Customer service response:\", outputs[0]['generated_text'].split(\"Customer service response: \")[1])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}